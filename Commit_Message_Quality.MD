# Commit Message Quality

## Purpose for This Review

**Commit messages** are the corresponding descriptions given by the developers when they push changes to online repositories.
To evaluate whether they are good enough in most situations or it's common to see bad messages that misleading reviewers (including project managers or other developers who would like to reuse the code), I conduct this review to find hints for following assumptions:

- **It's common to see poorly written commit messages.** If not, this research direction may not be valuable enough to pursue.
- **Recent studies in this area focus on small commits.** As far as I'm acknowledged, large commits are not taken good care of for now. The dataset for code summarization also usually only have small commits. Comprehending large commits is also a potential research problem to solve.
- **Whether code summarization tools generate comment/message accurately.** As I want to evaluate the human-written messages based on generated messages, the quality of generated messages is critical. If they are not accurate enough, the assumption *"generated messages are accurate"* or *"generated messages are more accurate than written ones"* won't stand.

## Most Relevant Papers Found

### \[ICSE-2022\] What Makes a Good Commit Message?

Link: <https://dl.acm.org/doi/pdf/10.1145/3510003.3510205>

**Major Contributions**: 

- They defined what is a "good" commit based on whether it summarizes the changes in this commit (what) and describe the reasons for the changes (why).
- They evaluate what proportion of commit messages lack information using a sample of almost 1,600 messages from five highly active open source projects and found that an average of around 44% of messages could be improved, suggesting using low-quality datasets may be a major threat when commit message generators are trained with such data.
- They proposed three classification models based on Bi-LSTM to automatically identify whether a commit message is well-written and whether a commit message contains “Why” (84.7% accuracy)or “What.” (91.0% accuracy)

### \[ICSE-NIER-2022\] Evaluating Commit Message Generation: To BLEU Or Not To BLEU?

Link: <https://arxiv.org/pdf/2204.09533>

**Key Points**: 

- This is a short paper mainly discussing what machine translation metrics (for example, BLEU) should be used to evaluate the quality of generated commit messages.
- The results of the experiments show the low correlation of BLEU with human evaluated scores, and they propose a novel metric, named *Log-MNEXT*.

### \[SANER-2021\] Quality Assurance for Automated Commit Message Generation

Link: <https://yanmeng.github.io/papers/SANER214.pdf>

**Key Points**

- **Quality issues exist for commit message generation task**. 
They claim that according to prior studies and our manual verification, approximately over 50% of commit messages generated by existing CMG approaches are semantically irrelevant to their reference messages.
This is more about concision rather than precision/accuracy.
- **A tool to identify filter out semantically irrelevant messages and preserve relevant ones.**
The tool is named QAcom, and it gives 69%~97% precision for semantically irrelevant and 78%~98% recall for semantically relevant messages.

## Additional Notes

- Is it possible to improve the commit message generation by adding architectural information?
- Is it possible to generation architectural-specific summaries for commits, and is this valuable?